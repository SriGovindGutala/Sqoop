Link for the sqoop documentation 
https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html

You can get help of all the sqoop commands 
sqoop import --help

Sqoop code follows the below principles
command subcommand series_of_arguments where command is sqoop, subcommand can be import or export and arguments are
--connect
--username
--password
--query
,etc.

Arguments are catogorized by 
•	Generic Arguments 
•	Import Arguments

Generic are
--connect 
--connection-manager 
--driver 
--hadoop-home (custom location)
--help
--p
–password (for mySQL or any database using connect)
–username 
–verbose 
–connection-param-file

Import are
--append	Append data to an existing dataset in HDFS
--as-avrodatafile	Imports data to Avro Data Files
--as-sequencefile	Imports data to SequenceFiles
--as-textfile	Imports data as plain text (default)
--boundary-query <statement>	Boundary query to use for creating splits
--columns <col,col,col…>	Columns to import from table
--direct	Use direct import fast path
--direct-split-size <n>	Split the input stream every n bytes when importing in direct mode
--inline-lob-limit <n>	Set the maximum size for an inline LOB
-m,--num-mappers <n>	Use n map tasks to import in parallel
-e,--query <statement>	Import the results of statement.
--split-by <column-name>	Column of the table used to split work units
--table <table-name>	Table to read
--target-dir <dir>	HDFS destination dir
--warehouse-dir <dir>	HDFS parent for table destination
--where <where clause>	WHERE clause to use during import
-z,--compress	Enable compression
--compression-codec <c>	Use Hadoop codec (default gzip)
--null-string <null-string>	The string to be written for a null value for string columns
--null-non-string <null-string>	The string to be written for a null value for non-string columns
